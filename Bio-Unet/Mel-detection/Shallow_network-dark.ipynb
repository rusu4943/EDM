{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3015e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda3\\Anaconda3\\envs\\XAI-2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import *\n",
    "import torchvision.models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c503170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(r\"../python_files/\")\n",
    "\n",
    "import python_files.utils as utils\n",
    "import python_files.dataloader_dark as dataloader\n",
    "import python_files.models as models\n",
    "import python_files.factory as factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac70fc3",
   "metadata": {},
   "source": [
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e29b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--report-freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--tune-mode', type=str, default='fine-tune', choices=['fine-tune', 'feature-extract'], help='tuning mode' )\n",
    "parser.add_argument('--backbone', type=str, default='resnet50', choices=['resnet50', 'vgg19', 'inception_v3'], \n",
    "                    help='backbone architecture' )\n",
    "parser.add_argument('--cls-type', type=str, default='single', choices=['single', \n",
    "                   'double', 'double-bn', 'double-dropout'], help='classifier architecture' )\n",
    "parser.add_argument('--hidden-dim', type=int, default=512, help='hidden dimension of classifier' )\n",
    "parser.add_argument('--record-root-dir', type=str, default='./record-data', help='record data root dir' )\n",
    "parser.add_argument('--exp', type=str, default='default_exp', help='name of experiment' )\n",
    "parser.add_argument('--batch-size', type=int, default=8, help='batch size' )\n",
    "parser.add_argument('--num-workers', type=int, default=4, help='number of processes working on cpu.')\n",
    "parser.add_argument('--num-classes', type=int, default=5, help='number of classes')\n",
    "parser.add_argument('--num-epochs', type=int, default=20,  help='number of epochs.')\n",
    "parser.add_argument( '--num-steps', type=int, default=-1, help='number of steps per epoch. '+ '-1 means use entire data' )\n",
    "parser.add_argument('--learn-rate', type=float, default=1e-3, help='learning rate for gradient descent')\n",
    "parser.add_argument('--weight-decay', type=float, default=1e-3, help='weight decay for optimization')\n",
    "parser.add_argument('--resume', action='store_true', help='resume experiment <exp> from last checkpoint' )\n",
    "parser.add_argument('--input-dir', type=str, default= r'../h5py/', help='data root dir' )\n",
    "parser.add_argument('--save-name', type=str, default='model.pt', help='saved model name' )\n",
    "\n",
    "args, unknown =  parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb4fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.num_epochs = 30\n",
    "args.learn_rate = 1e-4\n",
    "args.weight_decay = 1e-4 \n",
    "args.batch_size = 8\n",
    "args.input_dir1 = '../../Data/' \n",
    "args.backbone = 'resnet50'\n",
    "args.cls_type = 'single'\n",
    "args.exp = 'Mel_detection'\n",
    "args.save_name = 'Resnet50'\n",
    "args.num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564eab68",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50530767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(args):\n",
    "    name = args.exp\n",
    "    exp_dir = os.path.join( args.record_root_dir, name )\n",
    "    \n",
    "    log_format = '%(asctime)s %(message)s'\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "    fh = logging.FileHandler(os.path.join(exp_dir, 'log.txt'))\n",
    "    fh.setFormatter(logging.Formatter(log_format))\n",
    "    logging.getLogger().addHandler(fh)\n",
    "    log( f'Exp Name: {name}\\n\\n' )\n",
    "    log( f'Results will be stored in {exp_dir}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed35eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint.\n",
    "def load_experiment(args):\n",
    "    name = args.exp\n",
    "    exp_dir = os.path.join( args.record_root_dir, name )\n",
    "    \n",
    "    os.makedirs(args.record_root_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(exp_dir):\n",
    "        if not args.resume:\n",
    "            # if resume option is not specified\n",
    "            # check to make sure exp_dir is empty\n",
    "            files = os.listdir( exp_dir )\n",
    "            if len(files) > 1:\n",
    "                print( f'exp dir: {exp_dir} not empty. ' +\n",
    "                        'Please delete all files' +\n",
    "                        ' in dir and rerun train command.' )\n",
    "                #import pdb; pdb.set_trace()\n",
    "                exit( 1 )\n",
    "        else:\n",
    "            print('Hi')\n",
    "    else:\n",
    "        os.makedirs(exp_dir)\n",
    "    setup_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae8ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc02dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log( log_str ):\n",
    "    logging.info( log_str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1e4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_epoch_stats(prefix, batch_id, num_steps, loss, acc, mAUC, data_time, batch_time ):\n",
    "    \n",
    "    log( f'| {prefix} | ' +\n",
    "        f'EPOCH [{current_epoch+1:02d}/{num_epochs:02d}] ' +\n",
    "        f'Step [{batch_id+1:04d}/{num_steps:04d}] ' +\n",
    "        f'Loss: {loss:.2f} ' +\n",
    "        f'acc: {acc:.2f} ' +\n",
    "        f'mAUC: {mAUC:.2f} ' +\n",
    "        f'Data time: {data_time:.2f} ' +\n",
    "        f'Batch time: {batch_time:.2f} ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5d2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_auc_scores( auc_scores ):\n",
    "    ATTR_TO_INDEX = {\n",
    "    'Mel' : 0\n",
    "    }\n",
    "\n",
    "    INDEX_TO_ATTR = { idx:attr for idx, attr in ATTR_TO_INDEX.items() }\n",
    "    \n",
    "    \n",
    "    log_str = 'AUC Scores: '\n",
    "    for attr, idx in ATTR_TO_INDEX.items():\n",
    "        log_str += f'{attr}: {auc_scores:.2f} '\n",
    "    log_str += f'mAUC: {auc_scores.mean():.2f} '\n",
    "    log(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e01e7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, best_model, optimizer, current_epoch, args, save_name='model.pt'):\n",
    "    name = args.exp\n",
    "    save_name = args.save_name\n",
    "    exp_dir = os.path.join( args.record_root_dir, name )\n",
    "    \n",
    "    #Each epoch model save\n",
    "    save_path = os.path.join( exp_dir, save_name + '_model.pt' )\n",
    "    model_dict = model.state_dict()\n",
    "    opt_dict = optimizer.state_dict()\n",
    "    epoch = current_epoch+1\n",
    "    torch.save( { 'model': model_dict, 'opt': opt_dict, 'epoch': epoch }, save_path )\n",
    "    \n",
    "    # Best model\n",
    "    save_path = os.path.join( exp_dir, save_name + '_best_model.pt' )\n",
    "    model_dict = best_model.state_dict()\n",
    "    epoch = best_epoch\n",
    "    torch.save( { 'model': model_dict, 'epoch': epoch }, save_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77848b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats(train_loss,train_acc,train_auc_scores, train_data_time,\n",
    "               train_batch_time, val_loss, val_acc, val_auc_scores, val_data_time, val_batch_time, args):\n",
    "    name = args.exp\n",
    "    exp_dir = os.path.join( args.record_root_dir, name )\n",
    "    \n",
    "    save_path = os.path.join( exp_dir, 'stats.pt' )\n",
    "    stat_dict = {\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'train_auc_scores': train_auc_scores,\n",
    "        'train_data_time': train_data_time,\n",
    "        'train_batch_time': train_batch_time,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'val_auc_scores': val_auc_scores,\n",
    "        'val_data_time': val_data_time,\n",
    "        'val_batch_time': val_batch_time,\n",
    "        'args': args,\n",
    "    }\n",
    "    torch.save( stat_dict, save_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a131d4",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0d2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 256, x.H/4, x.W/4)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n",
    "        self.Avgpooling = nn.Sequential(self.base_layers[8])\n",
    "        \n",
    "        self.mlp = torch.nn.Sequential( torch.nn.Linear(2048,1), )# only one linear layer on top\n",
    "        \n",
    "    def forward(self, X):\n",
    "        layer0 = self.layer0(X)  # layer0:  torch.Size([1, 64, 256, 256])\n",
    "        layer1 = self.layer1(layer0) # layer1:  torch.Size([1, 256, 128, 128])\n",
    "        layer2 = self.layer2(layer1) # layer2:  torch.Size([1, 512, 64, 64])\n",
    "        layer3 = self.layer3(layer2) # layer3:  torch.Size([1, 1024, 32, 32])\n",
    "        layer4 = self.layer4(layer3) # layer4:  torch.Size([1, 2048, 16, 16])\n",
    "        out    = self.Avgpooling(layer4)\n",
    "        \n",
    "        out1 = torch.squeeze(out) \n",
    "        if X.size(0) == 1: # torch.Size([1, 2048, 1, 1]) => torch.Size([2048])\n",
    "            out1 = torch.unsqueeze(out1, 0) # torch.Size([1, 2048])\n",
    "        out = self.mlp(out1)\n",
    "        return out1, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d246a",
   "metadata": {},
   "source": [
    "# Streaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80648571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda3\\Anaconda3\\envs\\XAI-2\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "O:\\Anaconda3\\Anaconda3\\envs\\XAI-2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "net = Model()\n",
    "streaks_net = torch.nn.DataParallel(net)\n",
    "streaks_net = streaks_net.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60944f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "Main_folder = '../F(CLR) + F(Res) + F(Seg)/'\n",
    "record_data_folder = \"streaks/record-data/\"\n",
    "choosed_folder = 'SIMCLR+Resnet50+Unet/'\n",
    "model_name = 'SIMCLR+Resnet50+Unet_best_model.pt'\n",
    "streaks_Model_path = os.path.join(Main_folder, record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daa857d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(streaks_Model_path, map_location='cuda:0')\n",
    "streaks_net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8856e32",
   "metadata": {},
   "source": [
    "# Pigment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01ab2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()\n",
    "pigment_net = torch.nn.DataParallel(net)\n",
    "pigment_net = pigment_net.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51671c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "Main_folder = '../F(CLR) + F(Res) + F(Seg)/'\n",
    "record_data_folder = \"pigment/record-data/\"\n",
    "choosed_folder = 'SIMCLR+Resnet50+Unet/'\n",
    "model_name = 'SIMCLR+Resnet50+Unet_best_model.pt'\n",
    "pigment_Model_path = os.path.join(Main_folder, record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b021c794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(pigment_Model_path, map_location='cuda:0')\n",
    "pigment_net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd69db3",
   "metadata": {},
   "source": [
    "# Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de3486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()\n",
    "negative_net = torch.nn.DataParallel(net)\n",
    "negative_net = negative_net.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "705ddfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "Main_folder = '../F(CLR) + F(Res) + F(Seg)/'\n",
    "record_data_folder = \"negative/record-data/\"\n",
    "choosed_folder = 'SIMCLR+Resnet50+Unet/'\n",
    "model_name = 'SIMCLR+Resnet50+Unet_best_model.pt'\n",
    "negative_Model_path = os.path.join(Main_folder, record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd421b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(negative_Model_path, map_location='cuda:0')\n",
    "negative_net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cca6c7",
   "metadata": {},
   "source": [
    "# Milia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d99d15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()\n",
    "milia_net = torch.nn.DataParallel(net)\n",
    "milia_net = milia_net.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ebce09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "Main_folder = '../F(CLR) + F(Res) + F(Seg)/'\n",
    "record_data_folder = \"milia_like_cyst/record-data/\"\n",
    "choosed_folder = 'SIMCLR+Resnet50+Unet/'\n",
    "model_name = 'SIMCLR+Resnet50+Unet_best_model.pt'\n",
    "milia_Model_path = os.path.join(Main_folder, record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a5c809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(milia_Model_path, map_location='cuda:0')\n",
    "milia_net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e39b93",
   "metadata": {},
   "source": [
    "# Globules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47bdbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()\n",
    "globules_net = torch.nn.DataParallel(net)\n",
    "globules_net = globules_net.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "313de251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "Main_folder = '../F(CLR) + F(Res) + F(Seg)/'\n",
    "record_data_folder = \"globules/record-data/\"\n",
    "choosed_folder = 'SIMCLR+Resnet50+Unet/'\n",
    "model_name = 'SIMCLR+Resnet50+Unet_best_model.pt'\n",
    "globules_Model_path = os.path.join(Main_folder, record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afe7d85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(globules_Model_path, map_location='cuda:0')\n",
    "globules_net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95108b20",
   "metadata": {},
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee8e2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module): \n",
    "    def __init__(self, input_dim = 10240, output_dim = 1): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        #self.linear = torch.nn.Linear(input_dim, output_dim) \n",
    "        self.linear1 = torch.nn.Linear(input_dim, 2048) \n",
    "        self.linear2 = torch.nn.Linear(2048, 512) \n",
    "        self.linear3 = torch.nn.Linear(512, output_dim) \n",
    "        \n",
    "    def forward(self, x): \n",
    "        #outputs = torch.sigmoid(self.linear(x)) \n",
    "        output1 = self.linear1(x)\n",
    "        output2 = self.linear2(output1)\n",
    "        output3 = self.linear3(output2)\n",
    "        \n",
    "        return nn.Sigmoid()(output3).type(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01318b69",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5142737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear1): Linear(in_features=10240, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (linear3): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "model = LogisticRegression()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            params=filter( lambda p: p.requires_grad, model.parameters() ),\n",
    "            lr=args.learn_rate,\n",
    "            weight_decay=args.weight_decay )\n",
    "\n",
    "#init model\n",
    "criterion.to( DEVICE )\n",
    "model.to( DEVICE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd14ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model path\n",
    "record_data_folder = \"record-data/\"\n",
    "choosed_folder = 'Mel_detection/'\n",
    "model_name = 'Resnet50_best_model.pt'\n",
    "Logistic_Model_path = os.path.join(record_data_folder, choosed_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47c11d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(Logistic_Model_path, map_location='cuda:0')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95caa2",
   "metadata": {},
   "source": [
    "# Create list/seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3178850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp dir: ./record-data\\Mel_detection not empty. Please delete all files in dir and rerun train command.\n",
      "04/28 06:50:09 AM Exp Name: Mel_detection\n",
      "\n",
      "\n",
      "04/28 06:50:09 AM Results will be stored in ./record-data\\Mel_detection\n",
      "04/28 06:50:09 AM seed is: 43507\n",
      "04/28 06:50:09 AM args: Namespace(backbone='resnet50', batch_size=8, cls_type='single', exp='Mel_detection', hidden_dim=512, input_dir='../h5py/', input_dir1='../../Data/', learn_rate=0.0001, num_classes=1, num_epochs=30, num_steps=-1, num_workers=4, record_root_dir='./record-data', report_freq=10, resume=False, save_name='Resnet50', tune_mode='fine-tune', weight_decay=0.0001)\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "best_loss = np.inf\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "train_auc_scores = []\n",
    "train_data_time = []\n",
    "train_batch_time = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_auc_scores = []\n",
    "val_data_time = []\n",
    "val_batch_time = []\n",
    "\n",
    "\n",
    "# set seed\n",
    "seed = random.randint(0, 1e5)\n",
    "torch.manual_seed( seed )\n",
    "random.seed( seed )\n",
    "np.random.seed( seed )\n",
    "\n",
    "# load exp\n",
    "load_experiment(args)\n",
    "log( f'seed is: {seed}' )\n",
    "log( f'args: {args}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edfe6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = args.exp\n",
    "exp_dir = os.path.join( args.record_root_dir, name )\n",
    "args = args\n",
    "\n",
    "# dataloader\n",
    "num_classes = 1\n",
    "dataloader = dataloader.get_dataloader( args )\n",
    "\n",
    "# experiment params\n",
    "num_epochs = args.num_epochs\n",
    "current_epoch = 0\n",
    "best_epoch = 0\n",
    "report_freq = args.report_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785f706",
   "metadata": {},
   "source": [
    "# Train; Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "083c7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_5120(imgs):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        globules_net.eval()\n",
    "        negative_net.eval()\n",
    "        milia_net.eval()\n",
    "        pigment_net.eval()\n",
    "        streaks_net.eval()\n",
    "        \n",
    "        pred1, _ = globules_net( imgs )\n",
    "        pred2, _  = negative_net( imgs )\n",
    "        pred3, _  = milia_net( imgs )\n",
    "        pred4, _  = pigment_net( imgs )\n",
    "        pred5, _  = streaks_net( imgs )\n",
    "        \n",
    "        concate_input = torch.cat((pred1, pred2, pred3, pred4, pred5), 1)\n",
    "        \n",
    "    return concate_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58bc153c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28 06:50:12 AM | VAL | EPOCH [01/30] Step [0003/0003] Loss: 1.02 acc: 0.41 mAUC: 0.74 Data time: 0.31 Batch time: 0.89 \n",
      "04/28 06:50:12 AM AUC Scores: Mel: 0.74 mAUC: 0.74 \n"
     ]
    }
   ],
   "source": [
    "#********************val********************\n",
    "model.eval()\n",
    "loss_meter = utils.AverageMeter()\n",
    "acc_meter = utils.AverageMeter()\n",
    "data_time_meter = utils.AverageMeter()\n",
    "batch_time_meter = utils.AverageMeter()\n",
    "all_steps = len( dataloader[ 'dark' ] )\n",
    "num_steps = args.num_steps if args.num_steps > 0 else all_steps\n",
    "all_preds, all_labels = [], []\n",
    "start = time.time()\n",
    "# import pdb; pdb.set_trace()\n",
    "with torch.no_grad():\n",
    "    for batch_id, ( img_ids, imgs, labels, masks ) in enumerate(dataloader[ 'dark' ]):\n",
    "        data_time = time.time() - start\n",
    "        # break if we have iterated the required number of steps\n",
    "        if batch_id >= num_steps: break\n",
    "        # move data to device\n",
    "        batch_size = len( imgs )\n",
    "        imgs = imgs.to( DEVICE )\n",
    "        labels = labels.to( DEVICE )\n",
    "        # get model predictions\n",
    "        #pred = model( imgs )\n",
    "        concate_input = concate_5120(imgs)\n",
    "        pred = model( concate_input )\n",
    "        pred = torch.squeeze(pred,1) # [8,1]  => [8]\n",
    "\n",
    "        loss = criterion( pred, labels )\n",
    "        # stats\n",
    "        loss_meter.update( loss.item(), batch_size )\n",
    "        pred, labels = pred.detach().cpu(), labels.cpu()\n",
    "        acc = utils.accuracy( pred, labels )\n",
    "        acc_meter.update( acc, batch_size )\n",
    "        all_preds.append( pred )\n",
    "        all_labels.append( labels )\n",
    "        batch_time = time.time() - start\n",
    "        data_time_meter.update( data_time )\n",
    "        batch_time_meter.update( batch_time )\n",
    "        # log\n",
    "        if ( batch_id+1 ) % report_freq == 0:\n",
    "            log_epoch_stats( 'VAL', batch_id, num_steps, loss_meter.avg, acc_meter.avg, 0, data_time, batch_time )\n",
    "        start = time.time()\n",
    "\n",
    "val_loss.append( loss_meter.avg )\n",
    "val_acc.append( acc_meter.avg )\n",
    "val_batch_time.append( batch_time_meter.avg )\n",
    "val_data_time.append( data_time_meter.avg )\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "auc_scores = roc_auc_score(all_labels, all_preds, average=None)\n",
    "val_auc_scores.append( auc_scores )\n",
    "mean_auc_score = auc_scores.mean()\n",
    "\n",
    "log_epoch_stats( 'VAL', batch_id, num_steps, loss_meter.avg, acc_meter.avg, mean_auc_score,\n",
    "            data_time_meter.avg, batch_time_meter.avg )\n",
    "log_auc_scores( auc_scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c56d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
